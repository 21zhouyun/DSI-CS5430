Using custom data configuration default-afb2cac7f909cb5f
Reusing dataset json (cache/json/default-afb2cac7f909cb5f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 39.24it/s]
Using custom data configuration default-afb2cac7f909cb5f
Reusing dataset json (cache/json/default-afb2cac7f909cb5f/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 17.93it/s]
Using custom data configuration default-b928c18fed355c31
Reusing dataset json (cache/json/default-b928c18fed355c31/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 449.60it/s]
max_steps is given, it will override any value given in num_train_epochs
/home/qiyuan/.conda/envs/dsi/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 18000
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 4
  Gradient Accumulation steps = 1
  Total optimization steps = 100
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
wandb: Currently logged in as: hodavid538 (qyoo). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /home/qiyuan/DSI-CS5430/wandb/run-20240414_132647-xcxqjpbm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run results
wandb: ⭐️ View project at https://wandb.ai/qyoo/huggingface
wandb: 🚀 View run at https://wandb.ai/qyoo/huggingface/runs/xcxqjpbm
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<02:37,  1.59s/it]  2%|▏         | 2/100 [00:01<01:16,  1.28it/s]  3%|▎         | 3/100 [00:02<00:50,  1.92it/s]  4%|▍         | 4/100 [00:02<00:38,  2.49it/s]  5%|▌         | 5/100 [00:02<00:31,  2.98it/s]                                                 5%|▌         | 5/100 [00:02<00:31,  2.98it/s]  6%|▌         | 6/100 [00:02<00:27,  3.38it/s]  7%|▋         | 7/100 [00:02<00:25,  3.70it/s]  8%|▊         | 8/100 [00:03<00:23,  3.95it/s]  9%|▉         | 9/100 [00:03<00:22,  4.12it/s] 10%|█         | 10/100 [00:03<00:21,  4.26it/s]                                                 10%|█         | 10/100 [00:03<00:21,  4.26it/s]***** Running Evaluation *****
  Num examples = 18000
  Batch size = 4
wandb: - 0.012 MB of 0.012 MB uploadedwandb: \ 0.012 MB of 0.012 MB uploadedwandb: | 0.012 MB of 0.012 MB uploadedwandb: / 0.012 MB of 0.012 MB uploadedwandb: - 0.034 MB of 0.043 MB uploaded (0.002 MB deduped)wandb: \ 0.050 MB of 0.050 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:         train/epoch ▁▁
wandb:   train/global_step ▁█
wandb: train/learning_rate █▁
wandb:          train/loss █▁
wandb: 
wandb: Run summary:
wandb:         train/epoch 0.0
wandb:   train/global_step 10
wandb: train/learning_rate 0.00045
wandb:          train/loss 14.4714
wandb: 
wandb: 🚀 View run results at: https://wandb.ai/qyoo/huggingface/runs/xcxqjpbm
wandb: ️⚡ View job at https://wandb.ai/qyoo/huggingface/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2MjI4OTc4NQ==/version_details/v3
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240414_132647-xcxqjpbm/logs
Traceback (most recent call last):
  File "train.py", line 219, in <module>
    main()
  File "train.py", line 215, in main
    trainer.train()
  File "/home/qiyuan/.conda/envs/dsi/lib/python3.8/site-packages/transformers/trainer.py", line 1475, in train
    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)
  File "/home/qiyuan/.conda/envs/dsi/lib/python3.8/site-packages/transformers/trainer.py", line 1602, in _maybe_log_save_evaluate
    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)
  File "/home/qiyuan/.conda/envs/dsi/lib/python3.8/site-packages/transformers/trainer.py", line 2257, in evaluate
    output = eval_loop(
  File "/home/qiyuan/.conda/envs/dsi/lib/python3.8/site-packages/transformers/trainer.py", line 2431, in evaluation_loop
    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
  File "/home/qiyuan/DSI-CS5430/trainer.py", line 38, in prediction_step
    doc_ids = model.generate(
  File "/home/qiyuan/.conda/envs/dsi/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/qiyuan/.conda/envs/dsi/lib/python3.8/site-packages/transformers/generation_utils.py", line 1190, in generate
    return self.greedy_search(
  File "/home/qiyuan/.conda/envs/dsi/lib/python3.8/site-packages/transformers/generation_utils.py", line 1562, in greedy_search
    next_tokens_scores = logits_processor(input_ids, next_token_logits)
  File "/home/qiyuan/.conda/envs/dsi/lib/python3.8/site-packages/transformers/generation_logits_process.py", line 92, in __call__
    scores = processor(input_ids, scores)
  File "/home/qiyuan/.conda/envs/dsi/lib/python3.8/site-packages/transformers/generation_logits_process.py", line 525, in __call__
    mask[batch_id * self._num_beams + beam_id, self._prefix_allowed_tokens_fn(batch_id, sent)] = 0
  File "train.py", line 157, in restrict_decode_vocab
    valid_next_tokens = trie.get_valid_first_tokens(last_token)
UnboundLocalError: local variable 'last_token' referenced before assignment
